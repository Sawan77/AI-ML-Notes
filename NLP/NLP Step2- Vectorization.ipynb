{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F50G99nH112P"
      },
      "source": [
        "# Step2- Simple Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embedding is one such technique where we can represent the text using vectors. The more popular forms of word embeddings are:\n",
        "\n",
        "1. BoW (Bag of Words)\n",
        "2. TF-IDF (Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "97eqSgz_2MYK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88uW0zDh4BkP"
      },
      "source": [
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tZsw3i_L2K6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THBGyQba4Bcm"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81VT9JboTzt"
      },
      "source": [
        "# A) Basic Bag-of-Words (BOW)\n",
        "\n",
        "Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.\n",
        "\n",
        "A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_EAof8njfHz"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1IVdG29wyJ7"
      },
      "source": [
        "## 1) Plain frequency BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fwfWQDVyJpY"
      },
      "outputs": [],
      "source": [
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvS020Zzm6F"
      },
      "source": [
        "We want to build a basic bag-of-words (BOW) representation of our corpus. Based on what you now know from the lesson, you can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy.\n",
        "\n",
        "We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
        "https://scikit-learn.org/stable/index.html<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRhJPxbHwuj_"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAphZMVPBX9P"
      },
      "source": [
        "The *fit_transform* method does two things:\n",
        "1. It learns a vocabulary dictionary from the corpus.\n",
        "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5wi4_C7BAWv"
      },
      "outputs": [],
      "source": [
        "bow = vectorizer.fit_transform(corpus)\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bp1XNcF1FQ"
      },
      "source": [
        "We can take a look at the features and vocabulary dictionary.\n",
        "\n",
        "`Note-`**The CountVectorizer took care of tokenization and also removed punctuation and lower-cased everything.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQbqvLgVF8B7"
      },
      "outputs": [],
      "source": [
        "# View features (tokens).\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View vocabulary dictionary.\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "8navKcBfth9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmNUkZeExam"
      },
      "source": [
        "Specifically, the **CountVectorizer** generates a sparse matrix using an efficient, compressed representation. The sparse matrix object includes a number of useful methods:\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lug2-xnAExsb"
      },
      "outputs": [],
      "source": [
        "print(type(bow))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bywJ0XnGKPQ"
      },
      "source": [
        "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element represents a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At6Gt4bsEx2D"
      },
      "outputs": [],
      "source": [
        "print(bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1N1Io2EyAb"
      },
      "source": [
        "Before we explore further, we want to make a few modifications.\n",
        "1. What if we want to use another tokenizer like spaCy's?\n",
        "2. Instead of frequency, what if we want to have a binary BOW?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRgIHkzUVJtk"
      },
      "source": [
        "## 2) Binary BOW with custom tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tof1PBgqEy1D"
      },
      "source": [
        "**CountVectorizer** supports using a custom tokenizer. For every document, it will call your tokenizer and expect a list of tokens returned. We'll create a simple callback below which has spaCy tokenize and filter tokens, and then return them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcCLawrWEzC7"
      },
      "outputs": [],
      "source": [
        "# As usual, we start by importing spaCy and loading a statistical model.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
        "# the passed-in text and return the tokens, filtering out punctuation.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if not t.is_punct]\n",
        "\n",
        "spacy_tokenizer"
      ],
      "metadata": {
        "id": "0JgBmaZ4huZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEe1Lv_OScv"
      },
      "source": [
        "This time, we instantiate **CountVectorizer** with our custom tokenizer (*spacy_tokenizer*), turn off case-folding, and also set the *binary* parameter to *True* so we simply get 1s and 0s marking token presence rather than token frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YREyWzaA-rT"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
        "vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow = vectorizer.fit_transform(corpus)\n",
        "bow"
      ],
      "metadata": {
        "id": "30jCP_EOuEil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jDKQkZUOysa"
      },
      "source": [
        "Looking at the resulting feature names and vocabulary dictionary, we can see our *spacy_tokenizer* being used. If you're not convinced, you can remove the punctuation filtering in our tokenizer and rerun the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x6RBqTGq302"
      },
      "outputs": [],
      "source": [
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "aeNfBcT9uOH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFpQbdA-R3FI"
      },
      "source": [
        "To get a dense array representation of our sparse matrix, use *toarray*.<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray\n",
        "\n",
        "We can also index and slice into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yGr36aP9GCr"
      },
      "outputs": [],
      "source": [
        "print('A dense representation like we saw in the slides.')\n",
        "print(bow.toarray())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Indexing')\n",
        "print(bow[0])\n",
        "print()"
      ],
      "metadata": {
        "id": "AgYYhnsliNQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Indexing')\n",
        "print(bow[1])"
      ],
      "metadata": {
        "id": "UCCUmQasiO3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Slicing')\n",
        "print(bow[0:2])"
      ],
      "metadata": {
        "id": "wWxMMnQ7ios9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF0NVhdEUR1r"
      },
      "source": [
        "## 3) Cosine Similarity\n",
        "\n",
        "Cosine similarity quantifies the similarity between two vectors by measuring the cosine of the angle between them. This is particularly useful in text analysis, where texts are converted into vectors. Each dimension of the vector represents a word from the document, with its value indicating the frequency or importance of that word.\n",
        "\n",
        "When calculating cosine similarity, first, the dot product of the two vectors is found. This product gives a measure of how vectors in the same direction are aligned. Then, the magnitudes (or lengths) of each vector are calculated. The cosine similarity is the dot product divided by the product of the two vectors' magnitudes.\n",
        "\n",
        "This method effectively captures the orientation (or direction) of the vectors and not their magnitude, making it a reliable measure of similarity in texts of varying lengths. It's widely used in applications like recommendation systems, document clustering, and information retrieval, where understanding the similarity or dissimilarity between texts is crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI1VuDVVP4W"
      },
      "source": [
        "There are multiple ways to calculate Cosine similarity using scipy:\n",
        "<br><br>\n",
        "One way is using the **spatial** package, which is a collection of spatial algorithms and data structures. It has a method to calculate cosine *distance*. To get the cosine *similarity*, we have to substract the distance from 1.<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/spatial.html<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "w9VX1QLkvXze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQQ50IgXQfH"
      },
      "outputs": [],
      "source": [
        "# The cosine method expects array_like inputs, so we need to generate arrays from our sparse matrix.\n",
        "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
        "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
        "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
        "\n",
        "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
        "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
        "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SRDwr2gYD04"
      },
      "source": [
        "Another approach is using scikit-learn's *cosine_similarity* which computes the metric between multiple vectors. Here, we pass it our BOW and get a matrix of cosine similarities between each document.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwP8-jtchSI"
      },
      "outputs": [],
      "source": [
        "# cosine_similarity can take either array-likes or sparse matrices.\n",
        "print(cosine_similarity(bow))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarity(bow[0:2]))"
      ],
      "metadata": {
        "id": "FKC4XncWvgcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I96W6qDVdDnY"
      },
      "source": [
        "## 4) N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3E_hN5Ddyae"
      },
      "source": [
        "**CountVectorizer** includes an *ngram_range* parameter to generate different n-grams. n_gram range is specified using a minimum and maximum range. By default, n_gram range is set to (1, 1) which generates unigrams. Setting it to (1, 2) generates both unigrams and bigrams."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "7hdXez32xLrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZooyyRleHXe"
      },
      "outputs": [],
      "source": [
        "# Both unigrams and bigrams\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out(),\"\\n\")\n",
        "\n",
        "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())),\"\\n\")\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(bigrams)"
      ],
      "metadata": {
        "id": "LnspFXcUx6vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvtmi3negc0G"
      },
      "outputs": [],
      "source": [
        "# Setting n_gram range to (2, 2) generates only bigrams.\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out(),\"\\n\")\n",
        "print(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7e40ZAKhQmm"
      },
      "source": [
        "## Basic Bag-of-Words Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbdMO0bZjROn"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
        "# a list of tokens (each token's text) with punctuation filtered out.\n",
        "\n",
        "corpus = [\n",
        "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
        "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
        "  \"Aerial photography is a great way to identify terrestrial features that aren’t visible from the ground level, such as lake contours or river paths.\",\n",
        "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
        "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "    return [t.text for t in nlp(doc) if not t.is_punct]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
        "\n",
        "bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os3tPj5nmRLw"
      },
      "outputs": [],
      "source": [
        "# The string below is a whole paragraph. We want to create another # binary BOW but using the vocabulary\n",
        "# of our *current* CountVectorizer. This means that words in this paragraph which AREN'T already in the\n",
        "# vocabulary won't be represented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
        "# unless you rebuild your whole vocabulary. Still, we'll see that if there's enough overlapping vocabulary,\n",
        "# some similarity can still be picked up.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that we call 'transform' only instead of 'fit_transform' because the\n",
        "# fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
        "\n",
        "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
        "new_bow = vectorizer.transform(s)\n",
        "\n",
        "# EXERCISE: using the pairwise cosine_similarity method from sklearn, calculate the similarities between\n",
        "# each document from the corpus against this new document (new_bow).\n",
        "# HINT: You can pass two parameters to cosine_similarity in this case. See the docs:\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine\n",
        "\n",
        "# Which document is the most similar? Which is the least similar? Do the results make sense based on what you see?\n",
        "print(corpus)\n",
        "print()\n",
        "print(s)"
      ],
      "metadata": {
        "id": "4LN0xIOXjaXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(new_bow, bow[0])"
      ],
      "metadata": {
        "id": "oBOc2dFB0jmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(new_bow, bow[1])"
      ],
      "metadata": {
        "id": "9tV3vVc61MLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(new_bow, bow[2])"
      ],
      "metadata": {
        "id": "jwTe79iD1MPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(new_bow,bow[3])"
      ],
      "metadata": {
        "id": "x6JXEtSo1MSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All in one statement\n",
        "cosine_similarity(new_bow, bow[0:4])"
      ],
      "metadata": {
        "id": "RKL1PQsB1WJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghlqn6l-dal4"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: In spacy_tokenizer, instead of returning the plain text, return the lemma_ attribute instead.\n",
        "# How do the cosine similarity results differ? What if you filter out stop words as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnC_i4oH2ARW"
      },
      "source": [
        "# B) TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TF-IDF` stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set).\n",
        "\n",
        "**Term frequency (TF)**: number of times a term has appeared in a document. The term frequency is a measure of how frequently or how common a word is for a given sentence.\n",
        "\n",
        "**Inverse document frequency (IDF)**: is a measure of how rare a word is in a document. Words like “the”,” a” show up in all the documents but rare words will not occur in all the documents of the corpus.\n",
        "\n",
        "If a word appears in almost every document means it is not significant for the classification.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lfDKWtey2ZY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRtp9F8KS5QE"
      },
      "outputs": [],
      "source": [
        "#!pip install -U spacy==3.*\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMwv39AfP7Ti"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcTBtSx-XqZ"
      },
      "source": [
        "## Fetching datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYkq3i7_-qhQ"
      },
      "source": [
        "This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets of our own as well as fetch popular reference datasets online.<br>\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
        "<br><br>\n",
        "We'll use the **20 newsgroups** dataset, which is a collection of 18,000 newsgroup posts across 20 topics.<br>\n",
        "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
        "<br><br>\n",
        "List of datasets available:<br>\n",
        "https://scikit-learn.org/stable/datasets.html#datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjxqxVBBINV"
      },
      "source": [
        "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
        "<br><br>\n",
        "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9to6gQNCGiN"
      },
      "outputs": [],
      "source": [
        "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
        "                            remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus"
      ],
      "metadata": {
        "id": "6GXkcXe86Ne1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W989GHQxDvTW"
      },
      "source": [
        "We get back a **Bunch** container object containing the data as well as other information.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
        "<br><br>\n",
        "The actual posts are accessed through the *data* attribute and is a list of strings, each one representing a post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POGdVmdIDuCK"
      },
      "outputs": [],
      "source": [
        "print(type(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6AgmbL0ES9I"
      },
      "outputs": [],
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAjM4uNDEXGf"
      },
      "outputs": [],
      "source": [
        "# View first post.\n",
        "corpus.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View 2nd and 3rd post.\n",
        "corpus.data[1:3]"
      ],
      "metadata": {
        "id": "MqzXXJTo6gEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH99M6cxCpsz"
      },
      "source": [
        "## Creating TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtnQX-wWDhGh"
      },
      "outputs": [],
      "source": [
        "# Like before, if we want to use spaCy's tokenizer, we need\n",
        "# to create a callback. Remember to upgrade spaCy if you need\n",
        "# to (refer to beginnning of file for commentary and instructions).\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(nlp,\"\\n\")\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the\n",
        "# pipeline. We do need part-of-speech tagging however.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma (which require POS tagging).\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]\n",
        "\n",
        "spacy_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il-0gY9LEiNv"
      },
      "source": [
        "Like the classes to create raw frequency and binary bag-of-words vectors, scikit-learn includes a similar class called **TfidfVectorizer** to create TF-IDF vectors from a corpus.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "<br><br>\n",
        "The usage pattern is similar in that we call *fit_transform* on the corpus which generates the vocabulary dictionary (fit step), and generates the TF-IDF vectors (transform step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shj6BS0BN6FU"
      },
      "outputs": [],
      "source": [
        "# Use the default settings of TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = vectorizer.fit_transform(corpus.data)\n",
        "features"
      ],
      "metadata": {
        "id": "tISmK-6n7AUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ9w4gh9sobB"
      },
      "outputs": [],
      "source": [
        "# The number of unique tokens.\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CxmKlPcNRLk"
      },
      "outputs": [],
      "source": [
        "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJwnU8PZNdHU"
      },
      "outputs": [],
      "source": [
        "# What the encoding of the first document looks like in sparse format.\n",
        "print(features[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7VTwYzONlt"
      },
      "source": [
        "As we mentioned in the slides, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKLM-IMOwbJ"
      },
      "source": [
        "## Querying the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8oTtCg0QB71"
      },
      "source": [
        "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
        "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
        "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
        "3. Sort them in descending order by score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNjEUzqlP6Oy"
      },
      "outputs": [],
      "source": [
        "# Transform the query into a TF-IDF vector.\n",
        "query = [\"lunar orbit\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "print(query_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEfdfkmpP8Tv"
      },
      "outputs": [],
      "source": [
        "# Calculate the cosine similarities between the query and each document.\n",
        "# We're calling flatten() here becaue cosine_similarity returns a list of lists and we just want a single list.\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
        "#cosine_similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skuSFhLxXOMC"
      },
      "source": [
        "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0PvqRDpUSYO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy's argsort() method returns a list of *indices* that would sort an array:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "# The sort is ascending, but we want the largest k cosine_similarites at the bottom of the sort.\n",
        "# So we negate k, and get the last k entries of the indices list in reverse order.\n",
        "# There are faster ways to do this using things like argpartition but this is more succinct.\n",
        "\n",
        "def top_k(arr, k):\n",
        "  kth_largest = (k + 1) * -1\n",
        "  return np.argsort(arr)[:kth_largest:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFYpEldVUaAG"
      },
      "outputs": [],
      "source": [
        "# The top five documents.\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "print(top_related_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e86P3bQR1ZS"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at their respective cosine similarities.\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzdyTptURiTQ"
      },
      "outputs": [],
      "source": [
        "# Top match.\n",
        "print(corpus.data[top_related_indices[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQwWXypfR8vh"
      },
      "outputs": [],
      "source": [
        "# Second-best match.\n",
        "print(corpus.data[top_related_indices[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-5aqUbGSM5J"
      },
      "outputs": [],
      "source": [
        "# Try a different query\n",
        "query = [\"satellite\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "print(query_tfidf)\n",
        "\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "\n",
        "print(top_related_indices)\n",
        "print(cosine_similarities[top_related_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQtRQIcSbTj"
      },
      "outputs": [],
      "source": [
        "print(corpus.data[top_related_indices[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4v5wQ4JaBIh"
      },
      "source": [
        "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
        "<br>\n",
        "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
        "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
        "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
        "- Beyond word presence, intent and meaning are playing a larger role.\n",
        "<br>\n",
        "\n",
        "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Note:` sklearn provides 2 classes for implementing TF-IDF:\n",
        "1. Tfidftransformer where we need to compute word counts then compute IDF values and then compute the TF-IDF scores.\n",
        "2. Tfidfvectorizer here all the steps are done in a single step."
      ],
      "metadata": {
        "id": "v6Vn9svf3R92"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "## TF-IDF Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08nTQB7_fJU0"
      },
      "source": [
        "**EXERCISE**<br>\n",
        "Read up on these concepts we just mentioned if you're curious.<br>\n",
        "\n",
        "https://en.wikipedia.org/wiki/Inverted_index<br>\n",
        "https://en.wikipedia.org/wiki/Precision_and_recall<br>\n",
        "https://en.wikipedia.org/wiki/Okapi_BM25<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz2FCCq1fsjz"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: fetch multiple topics from the 20 newsgroups dataset and query them using the approach we followed.\n",
        "# A list of topics can be found here:\n",
        "# https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
        "#\n",
        "# If you're feeling ambitious, incorporate n-grams or look at how you can measure precision and recall."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}